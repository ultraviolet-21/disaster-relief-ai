{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ultraviolet-21/disaster-relief-ai/blob/main/DisasterRelief_Section1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiM6gYg0nhkY"
      },
      "source": [
        "<font color=\"#de3023\"><h1><b>REMINDER MAKE A COPY OF THIS NOTEBOOK, DO NOT EDIT</b></h1></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaxhytzBtGGt"
      },
      "source": [
        "# Disaster Relief Project Introduction\n",
        "\n",
        "In Section 1, we will build a very simply classifier using handwritten rules instead of machine learning. This might not be as powerful as a machine learning-based classifier but will give us a chance to understand the pipeline of building a classifier and to learn how to evaluate the classifier's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbW0_pxGhSKM"
      },
      "source": [
        "In this notebook we'll be:\n",
        "*   Going over Python Syntax\n",
        "*   Implementing Rule-Based Classifiers\n",
        "*   Evaluating using Confusion Matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8Jjcf09_fgd"
      },
      "source": [
        "### Discussion Section\n",
        "\n",
        "**Discuss:** Why might it be helpful to classify text during a disaster?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGy-5SuDyuOY"
      },
      "source": [
        "Let's import some libraries we will use!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8Fv_dgVytWC"
      },
      "source": [
        "#@title Load your dataset { display-mode: \"form\" }\n",
        "# Run this every time you open the spreadsheet\n",
        "\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import metrics\n",
        "from collections import Counter\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "!wget -q --show-progress 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Disaster%20Relief/disaster_data.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz1QW_jVDRGs",
        "cellView": "form"
      },
      "source": [
        "#@title If the previous cell fails to load data, use this cell\n",
        "import re\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torchtext.vocab import GloVe\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import requests, io, zipfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvQrvDA0_jED"
      },
      "source": [
        "## Load and Inspect the data\n",
        "\n",
        "To start off, let's take a closer look at the datasets we are providing you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6RA9qwTHPql"
      },
      "source": [
        "####Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0wD711p1sQl"
      },
      "source": [
        "# Load the data.\n",
        "disaster_tweets = pd.read_csv('disaster_data.csv',encoding =\"ISO-8859-1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZYPHoImuJuY"
      },
      "source": [
        "# This function prints out a table containing all the tweets, along with their category labels\n",
        "disaster_tweets.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt7-Kpl82-pQ"
      },
      "source": [
        "#Extract tweets and the respective labels from csv\n",
        "tweet_set = disaster_tweets['text']\n",
        "tweet_labels = disaster_tweets['category']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t56VfQdn9CmC"
      },
      "source": [
        "#Split the data as train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(tweet_set, tweet_labels, test_size=0.2, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1-rSQHz-GYb"
      },
      "source": [
        "**Discuss:** What are some interesting things you notice about the data? Do you agree with all of the labels?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9yEtwtoLE0u"
      },
      "source": [
        "### Is Our Data Balanced?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykqcVvNVLcXm"
      },
      "source": [
        "We are going to visualize how many tweets fall within each category."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate x labels for our plot\n",
        "tweet_categories = list(set(tweet_labels))\n",
        "\n",
        "# generate counts for each tweet type\n",
        "category_counts = [np.sum(disaster_tweets[\"category\"] == i) for i in tweet_categories]\n",
        "\n",
        "# generate a bar plot for our tweet labels that has different colors\n",
        "[plt.bar(x = tweet_categories[i], height = category_counts[i] ) for i in range(len(tweet_categories))]\n",
        "\n",
        "# make the plot interpretable with x and y labels + title\n",
        "plt.xlabel('TWEET CATEGORY')\n",
        "plt.ylabel('N OBSERVSATIONS')\n",
        "plt.title('A distribution of tweet categories in our data set', y=1.05);\n",
        "print(Counter(tweet_labels))"
      ],
      "metadata": {
        "id": "F0EknZ2ELDJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhXvkvh7Lwvo"
      },
      "source": [
        "Discuss the following questions based on your chart above.\n",
        "\n",
        "**Why is having a representative dataset important?**\n",
        "Consider from a perspective of ethics as well as classifier accuracy.\n",
        "\n",
        "\n",
        "**Is this dataset balanced for our purposes?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q9Nbw9juKui"
      },
      "source": [
        "### Coding Exercise\n",
        "\n",
        "Let's use some simple python to explore the data a bit more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkdal6lZtGG2"
      },
      "source": [
        "\n",
        "# The variable \"tweets\" is a list of tweets.\n",
        "# Mini-exercise 1: Print the number of tweets in the list\n",
        "print(\"Number of Tweets:\", len(disaster_tweets))\n",
        "\n",
        "# Mini-exercise 2: Assign the 10th tweet to the variable \"tweet\" and print it\n",
        "tweet_10 = disaster_tweets['text'][10]\n",
        "print(\"Tweet: \", tweet_10)\n",
        "\n",
        "# To view the category of a tweet, we access the attribute tweet.category\n",
        "# Mini-exercise 3: Print the category of this tweet.\n",
        "tweet_label_10 = disaster_tweets['category'][10]\n",
        "print(\"Tweet_Label: \",tweet_label_10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHc6eWG_AFC0"
      },
      "source": [
        "# Python Refreshers\n",
        "\n",
        "Exercises to refresh your memory on a few python concepts if needed!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWRsiQlmJrO6"
      },
      "source": [
        "### Functions\n",
        "\n",
        "(Check out your Python Basics notebook if you need!)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOVJy2qRACmh"
      },
      "source": [
        "A Python function is written like this:\n",
        "```\n",
        "def add_one(x):\n",
        "    return x+1\n",
        "```\n",
        "The name of the function is `add_one`, `x` is the input variable, and the `return` keyword tells us what to give as output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFevcJZvgB9p"
      },
      "source": [
        "### Coding Exercise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pm0cE3wtGG8"
      },
      "source": [
        "# Exercise 1. Define a function called \"square_minus_1\" that takes one variable (x),\n",
        "# squares it, subtracts 1, and returns the result.\n",
        "\n",
        "#### YOUR CODE STARTS HERE ####\n",
        "def square_minus_1(x):\n",
        "  return x**2-1\n",
        "#### YOUR CODE ENDS HERE ####\n",
        "\n",
        "\n",
        "### DO NOT EDIT BELOW THIS LINE.\n",
        "print(\"Testing:\")\n",
        "for x in [3,-4,6.5,0]:\n",
        "    print(str(x), \" -> \", str(square_minus_1(x)), end=' ')\n",
        "    print(\"CORRECT\" if square_minus_1(x)==(x**2-1) else \"INCORRECT\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_POgt22Jase"
      },
      "source": [
        "### If-else statements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxNYYxV0tGG_"
      },
      "source": [
        "An if/else statement looks like this:\n",
        "\n",
        "```\n",
        "if electoral_votes >= 270:\n",
        "    print(\"You win the election\")\n",
        "else:\n",
        "    print(\"You lose the election\")\n",
        "```\n",
        "\n",
        "The if-statement is evaluated (`electoral_votes >= 270`); if it's true then the code under the `if` is executed, if it's false then the code under the `else` is executed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h17vRtfLgPKr"
      },
      "source": [
        "### Coding Exercise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuXOcMC_tGHA"
      },
      "source": [
        "#Exercise 2. Define a function called \"contains_ss\" that takes one variable (word)\n",
        "# and returns True if the word contains a double-s and False if it doesn't.\n",
        "# Hint: to test whether a string e.g. \"ss\" is inside another string variable e.g. word, write\n",
        "#    if \"ss\" in word:\n",
        "\n",
        "#### YOUR CODE STARTS HERE ####\n",
        "def contains_ss(word):\n",
        "  if 'ss' in word:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "#### YOUR CODE ENDS HERE ####\n",
        "\n",
        "print(\"Testing:\")\n",
        "for word in [\"computer\", \"science\", \"lesson\"]:\n",
        "    print(\"{:s} ->\".format(word, contains_ss(word)), end=' ')\n",
        "    print(\"CORRECT\" if contains_ss(word)==(\"ss\" in word) else \"INCORRECT\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geK8mwMDIOfk"
      },
      "source": [
        "### More complex if-else statements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4e4UQHAtGHE"
      },
      "source": [
        "\n",
        "\n",
        "Maybe you want to check *several* conditions? You can use an if/elif/else statement.\n",
        "\n",
        "```\n",
        "if teamA_score > teamB_score:\n",
        "    print(\"Team A wins\")\n",
        "elif teamA_score < teamB_score:\n",
        "    print(\"Team B wins\")\n",
        "else:\n",
        "    print(\"It's a tie!\")\n",
        "```\n",
        "\n",
        "`elif` stands for \"else if\". In fact, the above code is just a neater way of writing this:\n",
        "```\n",
        "if teamA_score > teamB_score:\n",
        "    print(\"Team A wins\")\n",
        "else:\n",
        "    if teamA_score < teamB_score:\n",
        "        print(\"Team B wins\")\n",
        "    else:\n",
        "        print(\"It's a tie!\")\n",
        "```\n",
        "\n",
        "You can have as many `elif` statments as you like. These are useful for when you want several options."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crLQJazIIX2F"
      },
      "source": [
        "### Testing for equality and inequality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx-zj6KD-Bnt"
      },
      "source": [
        "Sometimes you want to check if two values are equal, perhaps using an `if` statement.\n",
        "To check for equality you need to use a double equals sign `==`.\n",
        "```\n",
        "x = 5\n",
        "y = 8\n",
        "if x == y:\n",
        "    print(\"x and y are equal\")\n",
        "```\n",
        "To check for *inequality*, i.e. if two things aren't equal, use `!=`.\n",
        "```\n",
        "x = 5\n",
        "y = 8\n",
        "if x != y:\n",
        "    print(\"x and y are NOT equal\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-XPuO4Oge16"
      },
      "source": [
        "### Coding Exercise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtFbIzbDD20X"
      },
      "source": [
        "# Exercise 3. Define a function called \"grade\" that takes one input (score).\n",
        "# If score >= 90, return the string \"A\"\n",
        "# Otherwise, if score >= 80, return the string \"B\"\n",
        "# Otherwise, if score >= 70, return the string \"C\"\n",
        "# Otherwise, if score >= 60, return the string \"D\"\n",
        "# Otherwise, if score >= 50, return the string \"E\"\n",
        "# Otherwise, return the string \"F\"\n",
        "\n",
        "#### YOUR CODE STARTS HERE ####\n",
        "def grade(score):\n",
        "  if score>=90:\n",
        "    return \"A\"\n",
        "  elif score >=80:\n",
        "    return \"B\"\n",
        "  elif score >=70:\n",
        "    return \"C\"\n",
        "  elif score >=60:\n",
        "    return \"D\"\n",
        "  elif score >=50:\n",
        "    return \"E\"\n",
        "  else:\n",
        "    return \"F\"\n",
        "#### YOUR CODE ENDS HERE ####\n",
        "\n",
        "\n",
        "print(\"Testing:\")\n",
        "for (score,g) in [(77,\"C\"),(80,\"B\"),(32,\"F\"),(100,\"A\"),(69,\"D\")]:\n",
        "    print(\"%f -> %s\" % (score, grade(score)))\n",
        "    print(\"CORRECT\" if grade(score)==g else \"INCORRECT\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPlUjrfV_1z5"
      },
      "source": [
        "## Iterators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNRMCtXrJuAr"
      },
      "source": [
        "### Lists"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUNHB8KG-31Q"
      },
      "source": [
        "In Python, a _list_ is an ordered collection of items. The items can be strings, numbers, booleans, or any other kind of Python object.\n",
        "\n",
        "You can create lists like this:\n",
        "```\n",
        "integer_list = [5, 6, 7, 8]\n",
        "string_list = ['hello', 'world']\n",
        "bool_list = [False, True, False, False, True]\n",
        "```\n",
        "\n",
        "If you want a list of the numbers up to (but not including) 10, you can use the `range` function.\n",
        "```\n",
        "upto10_list = range(10)\n",
        "```\n",
        "This gives you [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYkHSd6eJwca"
      },
      "source": [
        "### For loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAGtGi_w-31S"
      },
      "source": [
        "In Python, a _for loop_ allows you to iterate over a list.\n",
        "```\n",
        "shopping_list = ['bread', 'bananas', 'milk']\n",
        "\n",
        "for item in shopping_list:\n",
        "    print(item)\n",
        "```\n",
        "\n",
        "For example, the code above prints out the following output:\n",
        "\n",
        "```\n",
        "bread\n",
        "bananas\n",
        "milk\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTC2V7eRJzc0"
      },
      "source": [
        "### Incrementing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLreVp2Q_5pV"
      },
      "source": [
        "If you have an integer variable e.g. `x=3` and you want to increase `x` by 1 (which is called _incrementing_), then you can write\n",
        "```\n",
        "x = x+1\n",
        "```\n",
        "or, in shorthand:\n",
        "```\n",
        "x += 1\n",
        "```\n",
        "\n",
        "This can be useful when you're using `x` to count something. For example:\n",
        "```\n",
        "ages = [7, 14, 23, 3, 10, 19]\n",
        "\n",
        "num_adults = 0\n",
        "for age in ages:\n",
        "    if age >= 18:\n",
        "        num_adults += 1\n",
        "\n",
        "print(num_adults)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown What should the above code print out?\n",
        "\n",
        "answer = 0 #@param {type: \"number\"}\n",
        "\n",
        "if answer == 2:\n",
        "  print(\"Correct! The code prints out 2.\")\n",
        "else:\n",
        "  print(\"That's not quite it. Try again!\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gxuDmRJNoXYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w76hLvZu-LN2"
      },
      "source": [
        "### Coding Exercise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqsny2Fv-Bna"
      },
      "source": [
        "# Exercise 5.\n",
        "# Use a for-loop, incrementation and equality testing to count the number of cats in my list of pets.\n",
        "# Assign the result to the variable \"num_cats\"\n",
        "\n",
        "my_pets = ['cat', 'lizard', 'cat', 'dog', 'cat', 'snake', 'dog', 'cat', 'dog', 'parrot']\n",
        "\n",
        "\n",
        "#### YOUR CODE STARTS HERE ####\n",
        "\n",
        "num_cats = 0\n",
        "\n",
        "for pet in my_pets:\n",
        "    # Note: here we use the equality check (i.e. \"==\")\n",
        "    # rather than \"in\" (e.g. if \"cat\" in pet).\n",
        "    # Why?\n",
        "    # Hint: if we use \"in\", what would happen if I have a pet that's a \"catfish\"?\n",
        "    if pet == 'cat':\n",
        "        num_cats += 1\n",
        "\n",
        "#### YOUR CODE ENDS HERE ####\n",
        "\n",
        "print(\"Testing: num_cats = {:d}\".format(num_cats))\n",
        "print(\"CORRECT\" if num_cats==4 else \"INCORRECT\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le_HVtK8tGHI"
      },
      "source": [
        "# Rule-Based Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMDuot-SLv-M"
      },
      "source": [
        "Now we will build a rule-based classifier. This classifier will use handwritten rules (written by you!) to decide which category a tweet is part of. As you will see, this won't be super powerful or accurate (so don't worry if you are getting poor accuracy), but it will give you the chance to learn about the pipeline of building a classifier and how to evaluate how well it works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01wggohRhxWH"
      },
      "source": [
        "### Coding Exercise : Write a rule-based tweet classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA4S6UIGMazH"
      },
      "source": [
        "Time to write our rule-based classifier!\n",
        "The function outline below uses a `if/elif/else` statement to return the predicted category of a tweet.\n",
        "\n",
        "Fill in the missing `if` and `elif` statements with something sensible (there is no one right answer)!\n",
        "\n",
        "Start with something simple; we'll build it into something more complicated later.\n",
        "\n",
        "E.g. if \"sick\" in tweet:\n",
        "              return \"Medical\"\n",
        "\n",
        "*Hint: unless you come up with more complicated and inclusive conditions most predictions will be None, because most words you choose will not be in the tweets!*\n",
        "\n",
        "              \n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHhrx2jmtGHJ"
      },
      "source": [
        "def classify_rb(tweet):\n",
        "\n",
        "  tweet = str(tweet).lower() # this makes the tweet lower-case, so we don't have to worry about matching case\n",
        "\n",
        "  if \"medicine\" in tweet or \"first aid\" in tweet:\n",
        "    return \"Medical\"\n",
        "  elif \"power\" in tweet or \"battery\" in tweet:\n",
        "    return \"Energy\"\n",
        "  elif \"water\" in tweet or \"bottled\" in tweet:\n",
        "    return \"Water\"\n",
        "  elif \"food\" in tweet or \"perishable\" in tweet or \"canned\" in tweet:\n",
        "    return \"Food\"\n",
        "  else:\n",
        "    return \"None\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EnIMArJtGHU"
      },
      "source": [
        "##  Test your rule-based classifier on some examples\n",
        "\n",
        "Run the cell below to see the results of your rule-based classifier.\n",
        "You should see a table showing each tweet, along with its true category and the category predicted by your system.\n",
        "\n",
        "Which types of tweets does your system get right? Which types of tweets does your system get wrong and why?\n",
        "How would you measure the accuracy of your system?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAao0nAk62hl",
        "cellView": "form"
      },
      "source": [
        "#@title Helper function to show predictions\n",
        "def show_pred(y_test,y_pred):\n",
        "  table=pd.DataFrame([[t for t in X_test],y_pred, y_test]).transpose()\n",
        "  table.columns = ['Tweet', 'Predicted Category', 'True Category']\n",
        "  print(\"Percent Correct: %.2f\" % (sum(table['Predicted Category'] == table['True Category'])/len(table['True Category'])))\n",
        "  return table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JrvlPMqtGHW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "d67796f5-f5d7-4062-d9a7-a18772e5d628"
      },
      "source": [
        "#Use classify_rb to make predictions on the Test Data\n",
        "y_pred = [classify_rb(tweet) for tweet in X_test] # a list of predictions\n",
        "#Display the tweet with predicted and True category\n",
        "show_pred(y_test,y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-55d9929be7f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Use classify_rb to make predictions on the Test Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclassify_rb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# a list of predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#Display the tweet with predicted and True category\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mshow_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYOx7UNNNBR_"
      },
      "source": [
        "###Discussion Section 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PW7jrOO_ZvE"
      },
      "source": [
        "**Discuss:**\n",
        "*   What types of tweets does your classifier work well on?\n",
        "*   What does it not work well on?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iikyr-ktGHa"
      },
      "source": [
        "## Break your rule-based classifier!\n",
        "\n",
        "It's time to FOOL THE RULES!\n",
        "\n",
        "You'll be deliberately trying to break each others' rule-based classifiers by writing tricky tweets that fool your neighbor's rule-based classifier. Once your own classifier has been fooled by a tricky tweet, it's your job to amend the rules in your classifier to account for the new case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfHhW0MttGHe"
      },
      "source": [
        "### Exercises\n",
        "\n",
        "**Write a tweet about Food that will be misclassified.**\n",
        "\n",
        "Below, write a disaster-scenario tweet about Food that your classification function above will get wrong (i.e. fail to recognize it's about food).\n",
        "\n",
        "Hint: think of less-obvious food-related keywords that aren't included in the rule-based system above.\n",
        "\n",
        "Then run the cell - make sure the tweet is classified as something other than Food!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uXBwfCjtGHf"
      },
      "source": [
        "food_tweet = \"\"\n",
        "print(\"This tweet is classified as: {:s}\\n\".format(classify_rb(food_tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIurA2_ftGHl"
      },
      "source": [
        "**Write a tweet about Energy that will be misclassified**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Edi6eT3_tGHm"
      },
      "source": [
        "energy_tweet = \"\"\n",
        "print(\"This tweet is classified as: {:s}\\n\".format(classify_rb(energy_tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdwsfLr7tGHo"
      },
      "source": [
        "**Write a tweet about Water that will be misclassified**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTpR39EatGHp"
      },
      "source": [
        "water_tweet = \"\"\n",
        "print(\"This tweet is classified as: {:s}\\n\".format(classify_rb(water_tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8JJoAUltGHr"
      },
      "source": [
        "**Write a tweet about Medical that will be misclassified.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bJO_fQEtGHs"
      },
      "source": [
        "medical_tweet = \"\"\n",
        "print(\"This tweet is classified as: {:s}\\n\".format(classify_rb(medical_tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwLscHr9tGHv"
      },
      "source": [
        "**Write a tweet NOT about Food, that will be falsely classified as Food**\n",
        "\n",
        "Below, write a disaster-scenario tweet that is NOT about Food, but that the classifier above will classify as Food.\n",
        "\n",
        "Hint: you want to trick the classifier into thinking you're talking about food when you're not. Look at the keywords the rule-based system associates with food. Can you find a way to use them while actually talking about not-food?\n",
        "\n",
        "* For example, if the system looks for the word \"food\" you could write *Waiting out #Sandy by reading Plato. Food for thought.*\n",
        "* If the system looks for the word \"cook\", you could write *I hear the power's out in Cook County.*\n",
        "* More simply, you could mention food incidentally but the real subject of the tweet is something else e.g. *Was out food shopping when I heard about the power outage on the news. Hope everyone's OK.*\n",
        "\n",
        "Then run the cell - make sure the tweet is classified as Food!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU1ZC3votGHv"
      },
      "source": [
        "not_food_tweet = \"\"\n",
        "print(\"This tweet is classified as: {:s}\\n\".format(classify_rb(not_food_tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLPFZFOJtGH2"
      },
      "source": [
        "**Write a tweet NOT about Energy, that will be falsely classified as Energy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0On4JhCtGH5"
      },
      "source": [
        "not_energy_tweet = \"\"\n",
        "print(\"This tweet is classified as: {:s}\\n\".format(classify_rb(not_energy_tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2QIE0FGtGH8"
      },
      "source": [
        "**Write a tweet NOT about Water, that will be falsely classified as Water**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OplgXWCftGH-"
      },
      "source": [
        "not_water_tweet = \"\"\n",
        "print(\"This tweet is classified as: {:s}\\n\".format(classify_rb(not_water_tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iImOjUb_tGIC"
      },
      "source": [
        "**Write a tweet NOT about Medical, that will be falsely classified as Medical**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ3PwxPhkbcS"
      },
      "source": [
        "not_medical_tweet = \"\"\n",
        "print(\"This tweet is classified as: {:s}\\n\".format(classify_rb(not_medical_tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXBfVGAn9cTU"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "You might have heard about precision, recall, and accuracy scores during the first half of the program. Check out [this helpful blog post to learn more](https://towardsdatascience.com/precision-vs-recall-386cf9f89488).\n",
        "\n",
        "To evaluate our model, we'll want to understanding the following terms:\n",
        "\n",
        "* **TP (True Positive)** = The model predicted positive and it’s true.\n",
        "* **TN (True Negative)** = The model predicted negative and it’s true.\n",
        "* **FP (False Positive)** = The model predicted positive and it’s false.\n",
        "* **FN (False Negative)** = The model predicted negative and it’s false.\n",
        "\n",
        "\n",
        "\n",
        "###Common Metrics\n",
        "\n",
        "We'll eventually want to evaluate our model using a confusion matrix where we can extract commonly used metrics like:\n",
        "* **Precision** = TP / (TP + FP) - quantifies how often the things we classify as positive are actually positive.\n",
        "* **Recall** = TP / (TP + FN) - quantifies what fraction of actually positive examples we classify as positive.\n",
        "\n",
        "Finally, a useful score to summarize both precision and recall is the **F-1 score.** This is just a function (the harmonic mean) of precision and recall, shown in the summary below:\n",
        "\n",
        "<img src=\"https://datascience103579984.files.wordpress.com/2019/04/capture3-24.png\" width=\"400\" height=\"200\"></img>\n",
        "\n",
        "*Learn more about the F-1 score and other common metrics [here!](https://towardsdatascience.com/essential-things-you-need-to-know-about-f1-score-dbd973bf1a3)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtegHCjyOoa5"
      },
      "source": [
        "### Discussion Section 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib3UnuNNAhPE"
      },
      "source": [
        "**Discuss:**\n",
        "*   Name a situation in which recall would be more important than precision.\n",
        "*   Name a situation in which precision would be more important than recall.\n",
        "\n",
        "(*Hint: Remember the difference between false negatives and false positives*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUFqhPXEPATS"
      },
      "source": [
        "### Coding Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7Ej2P6cA2j8"
      },
      "source": [
        "\n",
        "\n",
        "Fill in the blanks in the code below to compute the precision, recall, and F1 scores of your rule-based classifier for individual classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94tE8YMbGebN"
      },
      "source": [
        "def evaluate(y_test,y_pred, c):\n",
        "    \"\"\"This function calculate the precision, recall and F1 for a single category c (e.g. Food)\n",
        "    Inputs:\n",
        "        predictions: a list of (tweet, predicted_category) pairs\n",
        "        c: a category\n",
        "    Returns:\n",
        "        The F1 score.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize variables to count the number of true positives, false positives and false negatives\n",
        "    true_positives = 0.0\n",
        "    true_negatives = 0.0\n",
        "    false_positives = 0.0\n",
        "    false_negatives = 0.0\n",
        "    print (len(y_test),len(y_pred))\n",
        "    # Iterate through the tweets, counting the number of true positives,true negatives, false positives and false negatives\n",
        "    for index,(true_category, predicted_category) in enumerate(zip(y_test,y_pred)):\n",
        "        #print (index,true_category,predicted_category)\n",
        "        # Hint: true positives for category c are tweets that have\n",
        "        # true category c and predicted category c\n",
        "        if true_category == c:\n",
        "          if (true_category == predicted_category):\n",
        "              true_positives += 1\n",
        "\n",
        "          # Finish the statement: false negatives for category c are tweets that have\n",
        "          # true category ___ and predicted category ___\n",
        "          else:# predicted_category != c:\n",
        "              false_negatives += 1\n",
        "        else:\n",
        "          # Finish the statement: false positives for category c are tweets that have\n",
        "          # true category ___ and predicted category ___\n",
        "          if predicted_category == c:\n",
        "              false_positives += 1\n",
        "          else:\n",
        "              true_negatives +=1\n",
        "\n",
        "\n",
        "    # Before we calculate Precision, Recall and F1 we need to check whether true_positives = 0. Why?\n",
        "    if true_positives == 0:\n",
        "        precision = 0.0\n",
        "        recall = 0.0\n",
        "        f1 = 0.0\n",
        "    else:\n",
        "        # Calculate Precision, Recall and F1\n",
        "        # Consult the formulae on the slides\n",
        "        precision = true_positives / ( true_positives + false_positives )\n",
        "        recall = true_positives / ( true_positives + false_negatives )\n",
        "        f1 = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "    # Print the category name, Precision, Recall and F1\n",
        "    print(c)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1:\", f1)\n",
        "    print()\n",
        "\n",
        "    # Return the F1 score\n",
        "    return f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbrd0rGIHgd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "0d2b3bd5-aab7-44d3-bba6-7d3acd2ae134"
      },
      "source": [
        "#Predictions for the entire dataset\n",
        "y_pred = [classify_rb(tweet) for tweet in X_test]\n",
        "# Get the F1 scores for each category\n",
        "food_f1 = evaluate(y_test,y_pred, \"Food\")\n",
        "water_f1 = evaluate(y_test,y_pred, \"Water\")\n",
        "energy_f1 = evaluate(y_test,y_pred, \"Energy\")\n",
        "medical_f1 = evaluate(y_test,y_pred, \"Medical\")\n",
        "none_f1 = evaluate(y_test,y_pred, \"None\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-10dbd6dd1503>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Predictions for the entire dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclassify_rb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Get the F1 scores for each category\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfood_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Food\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwater_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Water\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJzhPG8eEW0u"
      },
      "source": [
        "### F1 Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpNZKvBxtR64"
      },
      "source": [
        "Complete the cell below to calculate the average F1 score, which should be the average of the F1 scores for each category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FgHyD3ttR65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "72e55819-edd7-411c-d28e-c07b1e8d2927"
      },
      "source": [
        "average_f1 = (food_f1 + energy_f1 + medical_f1 + water_f1 + none_f1) / 5\n",
        "print(\"Average F1:\", average_f1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9b48425737bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maverage_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfood_f1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0menergy_f1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmedical_f1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwater_f1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnone_f1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average F1:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_f1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'food_f1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyH5sEizHufO"
      },
      "source": [
        "###Using Sklearn to compute Precision, Recall, and F1-score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw31k9oYHtl4"
      },
      "source": [
        "print(classification_report(y_test, y_pred, target_names=['Energy', 'Food', 'Medical', 'None', 'Water']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_klzFhMPMS1"
      },
      "source": [
        "## Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWeBnogUtR7A"
      },
      "source": [
        "\n",
        "A confusion matrix is a way to visualize the performance of the classifier on different categories. Here is an example of a confusion matrix for a model that predicts flower species:\n",
        "\n",
        "<img src=\"https://editor.analyticsvidhya.com/uploads/812455.jpg\" width=\"600\" height=\"400\"></img>\n",
        "\n",
        "* _Rows_ represent the _true category_ (of the tweet, for our model)\n",
        "* _Columns_ represent the _predicted category_ from your classifier\n",
        "* So numbers on the diagonal represent correct classifications, and off-diagonal numbers represent misclassification.\n",
        "\n",
        "\n",
        "We have written the code to compute the confusion matrix for you, so you don't have to do any extra work. But please feel free to read through the code so that you understand what's going on!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9L8C5RZtR7B",
        "cellView": "form"
      },
      "source": [
        "#@title Helper Function-Confusion Matrix\n",
        "'''\n",
        "Plots the confusion Matrix and saves it\n",
        "'''\n",
        "def plot_confusion_matrix(y_true,y_predicted):\n",
        "  cm = metrics.confusion_matrix(y_true, y_predicted)\n",
        "  print (\"Plotting the Confusion Matrix\")\n",
        "  labels = ['Energy', 'Food', 'Medical', 'None', 'Water']\n",
        "  df_cm = pd.DataFrame(cm,index =labels,columns = labels)\n",
        "  fig = plt.figure()\n",
        "  res = sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')\n",
        "  plt.yticks([0.5,1.5,2.5,3.5,4.5], labels,va='center')\n",
        "  plt.title('Confusion Matrix - TestData')\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3mImyOpPdcj"
      },
      "source": [
        "###Discussion Section 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3m7ZCqzCZ0r"
      },
      "source": [
        "**Discuss:**\n",
        "*   Explain what each of the cells  in the confusion matrix table mean.\n",
        "*   Which category do you think your classifier is doing the best on?\n",
        "*   Which is it doing the worst on?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "164do05lPxvb"
      },
      "source": [
        "###Confusion Matrix Heatmap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hxm47gWtR75"
      },
      "source": [
        "We can also display the confusion matrix as a *heatmap*.  Run the following code to display this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jv2BUA8tR76"
      },
      "source": [
        "plot_confusion_matrix(y_test,y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp5FSIWtP-zN"
      },
      "source": [
        "###Discussion Section 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-xgOf6hDPqF"
      },
      "source": [
        "Can you explain what it means when a color is darker vs lighter?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7F7W4xotGIK"
      },
      "source": [
        "Bonus: [The Cat and Mouse Game](http://tangra.cs.yale.edu/naclo/practice/2012A.html)"
      ]
    }
  ]
}